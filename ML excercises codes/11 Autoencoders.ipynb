{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de Exerc.12 - Autoencoders ","version":"0.3.2","provenance":[{"file_id":"1HTUmaNlBtjAFTiomsrdB09oM8DT6DLqY","timestamp":1543863009144},{"file_id":"1Q6WIXhkNJmgzJskDdYoKv2mxtFP7FX3s","timestamp":1543862732578}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"m_XHTOYUex1g","colab_type":"text"},"cell_type":"markdown","source":["# Notebook 12 - Autoencoders.\n","\n","*   Recuerda que puedes consultar la documentación sobre una función escribiendo **?** justo después de la función: *Ejemplo: np.maximum?*\n","*   Puedes ejecutar el contenido de una celda con el atajo de teclado **CTRL+ENTER**\n","*   Utiliza **TAB** cada vez que quieras autocompletar una llamada a una función.\n","*   Puedes ejecutar instrucciones de bash directamente desde el notebook usando **!** : *Ejemplo: !pip install tensorflow*\n","*   Recuerda que Google es tu amigo, y saber buscar la información en las documentaciones de las librerías es muy importante.\n","*   Una solución correcta no es la que funciona sino la que se entiende!\n","*   No dudes en preguntar cualquier duda al profesor que lleva todo el día dando la turra."]},{"metadata":{"id":"d_bnlqtPZ4bz","colab_type":"text"},"cell_type":"markdown","source":["## 1. Clasificando imágenes reales (CIFAR100 - Dataset)  [Ejerc. 2 - Notebook 11]\n","\n","---\n","\n","**Tarea:** Entrena y valida un modelo de Redes Neuronales Convolucionales sobre el dataset CIFAR-100 incluido dentro de ***tf.keras.datasets.cifar100***. Una vez tengas tu modelo correctamente entrenado, prueba a visualizar los filtros aprendidos en la primera capa. Recuerda que nunca está de más hacer un análisis exploratorio de los datos para ganar más conocimiento de con qué estamos trabajando.\n","\n","**Ojo!** Ten presente que ahora estaremos trabajando con imágenes a color, así que la dimensión correspondiente al canal, será de tamaño 3, en referencia a las tres intensidades de color **RGB** (Red, Blue, Green) "]},{"metadata":{"id":"QBoQvGOqX4yC","colab_type":"code","outputId":"18546a5f-a3f5-4bf5-f526-20043f925346","executionInfo":{"status":"ok","timestamp":1544028812808,"user_tz":0,"elapsed":81653,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["import numpy as np\n","import scipy as sc\n","import sklearn as sk\n","import pandas  as pd\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.layers     import Dense\n","from tensorflow.keras.utils      import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","\n","from tensorflow.keras.datasets import cifar100\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Cargamos el dataset desde la función.\n","train_data, test_data = cifar100.load_data(\"fine\") \n","\n","X_train, Y_train = train_data\n","X_test, Y_test   = test_data\n","\n","\n","\n","# Guardamos las dimensiones de la matriz de entrada.\n","n = X_train.shape[0]\n","\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train /= 255.0\n","X_test /= 255.0\n","\n","Y_train = to_categorical(Y_train, 100)\n","Y_test = to_categorical(Y_test, 100)\n","\n","\n","print(\"Dimensiones de datos de entrenamiento:\", X_train.shape, \" Y:\", Y_train.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","169009152/169001437 [==============================] - 76s 0us/step\n","Dimensiones de datos de entrenamiento: (50000, 32, 32, 3)  Y: (50000, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"mtkncQ7ufGwC","colab_type":"code","outputId":"8544856c-1696-4f65-805e-98ba7713e54c","executionInfo":{"status":"ok","timestamp":1544005033115,"user_tz":0,"elapsed":1081,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(X_train[0].max(), X_train[0].min())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.0 0.003921569\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"KtYKqFZ0abwD","colab":{}},"cell_type":"code","source":["from tensorflow.python.keras.models import Sequential, Model\n","from tensorflow.python.keras.layers import InputLayer, Input\n","from tensorflow.python.keras.layers import Reshape, MaxPooling2D, BatchNormalization\n","from tensorflow.python.keras.layers import Conv2D, Dense, Flatten, Conv2DTranspose, Dropout\n","from tensorflow.python.keras import optimizers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oaoOXMIvasyv","colab_type":"code","colab":{}},"cell_type":"code","source":["'''input1 = Input(shape=(32,32,3))\n","\n","norm = BatchNormalization()(input1)\n","conv1 = Conv2D(filters=32, kernel_size=4, padding='same', activation='relu')(norm)\n","pool1 = MaxPooling2D(pool_size=2, padding='valid')(conv1)\n","conv2 = Conv2D(filters=64, kernel_size=4, padding='same', activation='relu')(pool1)\n","conv3 = Conv2D(filters=128, kernel_size=8, activation='relu')(conv2)\n","\n","pool2 = MaxPooling2D(pool_size=2, padding='valid')(conv2)\n","\n","flatten = Flatten()(pool2)\n","out = Dense(100, name='out_layer')(flatten)\n","\n","model = Model(inputs=input1 , outputs=out)'''\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"djfXjvcpkGbR","colab_type":"code","colab":{}},"cell_type":"code","source":["'''model = Sequential()\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=32, kernel_size=(4,4), padding='same', activation='relu'))\n","model.add(Dropout(0.4))\n","model.add(Conv2D(filters=32, kernel_size=(8,8), padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=2, padding='valid'))\n","\n","model.add(Dropout(0.4))\n","model.add(Conv2D(filters=64, kernel_size=(8,8), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(16,16), padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=2, padding='valid'))\n","\n","model.add(Flatten())\n","model.add(Dense(512, name='other', activation='relu'))\n","model.add(Dense(100, name='out_layer', activation='softmax'))'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7KgPlY33wsAk","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='elu'))\n","model.add(MaxPooling2D(pool_size=2))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.5))\n","\n","model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='elu'))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization())\n","# model.add(Dropout(0.5))\n","\n","model.add(Conv2D(filters=512, kernel_size=(5, 5), padding='valid', activation='elu'))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization())\n","\n","model.add(Flatten())\n","model.add(Dense(512, name='other', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, name='other2', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(100, name='out_layer', activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J1CkWCu-AViw","colab_type":"code","outputId":"3a9c599a-2dc2-4b42-ae0e-73a664336fc3","executionInfo":{"status":"ok","timestamp":1544005037727,"user_tz":0,"elapsed":587,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from keras.regularizers import l2\n","L2_DECAY_RATE = 0.0005\n","\n","# https://github.com/geifmany/cifar-vgg/blob/master/cifar100vgg.py\n","\n","model = Sequential()\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='elu', kernel_regularizer=l2(L2_DECAY_RATE)))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='elu', kernel_regularizer=l2(L2_DECAY_RATE)))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(filters=512, kernel_size=(5, 5), padding='same', activation='elu', kernel_regularizer=l2(L2_DECAY_RATE)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(filters=512, kernel_size=(5, 5), padding='same', activation='elu', kernel_regularizer=l2(L2_DECAY_RATE)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(filters=1024, kernel_size=(5, 5), padding='same', activation='elu', kernel_regularizer=l2(L2_DECAY_RATE)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(512, name='other', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, name='other2', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(100, name='out_layer', activation='softmax'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"HJPtNWSZeED0","colab_type":"code","outputId":"1b7f9bb2-8884-4144-feb5-f0a7eedc5504","executionInfo":{"status":"error","timestamp":1544029825888,"user_tz":0,"elapsed":1337,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":489}},"cell_type":"code","source":["from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","\n","sgd = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.95, nesterov=True)\n","adam = optimizers.Adam(lr=0.02)\n","\n","model.compile(loss='categorical_crossentropy',\n","                optimizer=sgd,\n","                metrics=['mse', 'accuracy'])\n","\n","model.load_weights('./top_weights .h5')\n","\n","mc = ModelCheckpoint(filepath='./top_weights cis100.h5', monitor='val_acc', save_best_only='True', save_weights_only='True', verbose=1)\n","es = EarlyStopping(monitor='val_acc', patience=15, verbose=1)\n","rlr = ReduceLROnPlateau(monitor='val_acc')\n","\n","callbacks = [mc, es, rlr]\n","\n","history = model.fit(X_train, Y_train, epochs=40, batch_size=128,\n","                    validation_data=(X_test, Y_test), verbose=True,\n","                    callbacks=callbacks)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-b30b3224dc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 metrics=['mse', 'accuracy'])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./top_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./top_weights cis100.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    802\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    805\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer #2 (named \"batch_normalization\" in the current model) was found to correspond to layer encoder_conv3 in the save file. However the new layer batch_normalization expects 4 weights, but the saved weights have 2 elements."]}]},{"metadata":{"id":"Y-zzCiGCla01","colab_type":"code","outputId":"ed91c729-7007-40d8-c289-31e3265879b0","executionInfo":{"status":"ok","timestamp":1544013390997,"user_tz":0,"elapsed":482,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              multiple                  1792      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) multiple                  0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            multiple                  409856    \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            multiple                  3277312   \n","_________________________________________________________________\n","batch_normalization (BatchNo multiple                  2048      \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            multiple                  6554112   \n","_________________________________________________________________\n","batch_normalization_1 (Batch multiple                  2048      \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            multiple                  13108224  \n","_________________________________________________________________\n","batch_normalization_2 (Batch multiple                  4096      \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","flatten (Flatten)            multiple                  0         \n","_________________________________________________________________\n","other (Dense)                multiple                  524800    \n","_________________________________________________________________\n","dropout (Dropout)            multiple                  0         \n","_________________________________________________________________\n","other2 (Dense)               multiple                  131328    \n","_________________________________________________________________\n","dropout_1 (Dropout)          multiple                  0         \n","_________________________________________________________________\n","out_layer (Dense)            multiple                  25700     \n","=================================================================\n","Total params: 24,041,316\n","Trainable params: 24,037,220\n","Non-trainable params: 4,096\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"pSy99Ir8vdry","colab_type":"code","outputId":"c0ee24d2-aa3a-4aad-c170-272e75ae4a91","executionInfo":{"status":"ok","timestamp":1544028976783,"user_tz":0,"elapsed":3995,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":592}},"cell_type":"code","source":["weight = np.array(model.layers[0].get_weights())\n","filter1 = weight[0][:,:,:,0]\n","\n","def showFilters(filters):\n","  \n","  fig, axes = plt.subplots(8, 8, figsize = (10,10))\n","  \n","  for i in range(64):\n","    axesx = int(i/8)\n","    axesy = i % 8\n","    filter1 = filters[:,:,:,i]\n","    rescale = (filter1 + filter1.min()) / (filter1.max() - filter1.min())\n","    axes[axesx, axesy].imshow(np.clip(rescale, 0,1))\n","    axes[axesx, axesy].axis('off')\n","    \n","showFilters(weight[0])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAk4AAAI/CAYAAACBJ1aRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f2666632ba8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Q04jc_4ZuQh1","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.matshow(image.reshape(28, 28))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g4MJO4UiaCbH","colab_type":"text"},"cell_type":"markdown","source":["## 2. Autoencoders y espacios latentes con MNIST.\n","\n","No, si ya me sabe mal volver al maldito dataset MNIST, pero es que... ya conociéndolo podemos aprovechar mucho del código que hemos desarrollado en ejercicios anteriores para trabajar más rápido. En cualquier caso, si lo prefieres, puedes trabajar con otros datasets similares. Por ejemplo, puedes probar a trabajar con el Fashion MNIST (de complementos de moda) o notMNIST (de diferentes tipografías).\n","\n","Hoy lo que vamos a hacer es implementar nuestro propio ***autoencoder***, y con el, extraer vectores de nuestro espacio latente. Al turrón!\n","\n","---\n","\n","**Tarea:** Lo primero en lo que tienes que trabajar es en diseñar un modelo ***autoencoder*** sencillo, que sólamente disponga de una capa oculta para el *encoder* y otra para el *decoder*. Recuerda que deberás crear tu modelo de la misma forma que hemos hecho siempre, pero además crear modelos aparte utilizando las capas del primer modelo construido para poder trabajar con el ***encoder*** y el ***decoder*** de manera independiente.\n","\n","**Pregunta** : ¿Qué función de activación crees que es mejor elegir para utilizar en la última capa del ***decoder***: *ReLU*, *Sigmoide* o *Softmax*?¿Por qué? Pista : La solución rima con *palanganoide*. \n","\n","Una vez tengas tu *autoencoder* construido y funcionando realiza lo siguiente:\n","\n","1. Haz que tu autoencoder sólo tenga dos neuronas en su cuello de botella y entrena al modelo. Una vez lo tengas, prueba a visualizar una imagen de entrada y el output reconstruido por la red. ¿Es una buena reconstrucción? Si no estás convencido del resultado, comprueba que el autoencoder ha hecho su trabajo, visualizando un *** scatter plot*** donde cada variable latente estará en un eje, y donde cada clase se visualice de un color. Si tu modelo es correcto, deberías de observar cómo números de clases similares estarán en posiciones cercanas formando clusters.\n","\n","2. Ahora repite el entrenamiento del autoencoder pero utilizando 16 neuronas en su cuello de botella. Ahora sí deberías de ser capaz de obtener una buena reconstrucción de tus imágenes. ¿Podemos intentar observar si la representación en el espacio latente está ordenada? Vamos a generar una interpolación. Prueba a codificar en tu *encoder* dos imágenes diferentes y guarda sus vectores codificados en el espacio latente. Ahora, genera interpolaciones de esos dos vectores (es decir, mézclalos  en diferentes proporciones) y decodifícalos con tu ***decoder***. ¿Observas algún tipo de interpolación?\n","\n","3. (**Opcional**) Prueba a añadir ruido al *input* (utiliza la función implementada hace un par de ejercicios) y entrena a tu ***autoencoder*** para comprobar si tu modelo es capaz de actuar como un *denoiser*."]},{"metadata":{"id":"K-KoRLswf3__","colab_type":"code","outputId":"3443a9e3-97a2-467b-98c2-6950c84b1bc1","executionInfo":{"status":"ok","timestamp":1543869531178,"user_tz":0,"elapsed":3466,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["# Cargamos el dataset desde el archivo.\n","mnist = pd.read_csv(\"./sample_data/mnist_train_small.csv\", header=None).as_matrix()\n","\n","# Guardamos las variables X e Y.\n","X, Y = mnist[:, 1:], mnist[:, 0:1]\n","\n","n, p = X.shape\n","\n","# Normalizamos input y codificamos output con one-hot encoding.\n","Xt = X / 255.0\n","Yt = to_categorical(Y, 10)\n","\n","Xt = Xt.reshape(n, 28, 28, 1)\n","\n","# Generamos train y test set.\n","X_train, X_test, Y_train, Y_test = train_test_split(Xt, Yt, train_size=0.7)\n","\n","\n","##"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n","  FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"od6YnzlVyvoB","colab_type":"code","outputId":"397f9435-562a-48e8-b4b7-441e03043285","executionInfo":{"status":"ok","timestamp":1543869532367,"user_tz":0,"elapsed":656,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["X_train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14000, 28, 28, 1)"]},"metadata":{"tags":[]},"execution_count":55}]},{"metadata":{"id":"0dDHinubnwVX","colab_type":"code","colab":{}},"cell_type":"code","source":["inputs = Input(shape=(28, 28, 1))\n","conv1 = Conv2D(filters=32, kernel_size=5, strides=2, padding='same', activation='relu', name='capa1')(inputs)\n","conv2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='capa2')(conv1)\n","conv3 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name='capa3')(conv2)\n","\n","flatten = Flatten()(conv3)\n","code = Dense(2, activation='sigmoid')(flatten)\n","\n","code2 = Dense(3* 3* 128, activation='sigmoid')(code)\n","\n","reshape = Reshape((3, 3, 128))(code2)\n","\n","dconv3 = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='dconv3')(reshape)\n","dconv2 = Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same', activation='relu', name='dconv2')(dconv3)\n","dconv1 = Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='sigmoid', name='dconv1')(dconv2)\n","\n","model2 = Model(inputs=inputs, outputs=dconv1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EstbPBLIx5TH","colab_type":"code","outputId":"547674a7-26b2-4329-fc6e-a61877d11be4","executionInfo":{"status":"error","timestamp":1543869648567,"user_tz":0,"elapsed":1105,"user":{"displayName":"Alberto Xirzag","photoUrl":"https://lh5.googleusercontent.com/-Wb_I_y4NJ-8/AAAAAAAAAAI/AAAAAAAAADo/qvmLmJUcT1A/s64/photo.jpg","userId":"02782791740644802739"}},"colab":{"base_uri":"https://localhost:8080/","height":693}},"cell_type":"code","source":["model2.compile(loss='mse',\n","              optimizer=adam,\n","              metrics=['mse', 'accuracy'])\n","\n","model2.fit(X_train, X_train, epochs=100, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-7100b72faddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['mse', 'accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1152\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    333\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dconv1 to have shape (3, 3, 1) but got array with shape (28, 28, 1)"]}]}]}